{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12737a5f-7aaf-44d7-9025-52b7f35a0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from random import sample\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f0196-dd22-4bff-9775-4f8c4d093819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefined for the project as the task is considered as classification in patches\n",
    "# some constants\n",
    "PATCH_SIZE = 16  # pixels per side of square patches\n",
    "VAL_SIZE = 10  # size of the validation set (number of images)\n",
    "CUTOFF = 0.25  # minimum average brightness for a mask patch to be classified as containing road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K = 6 # constant, keep it\n",
    "RANDOM_STATE = 100 # constant, keep it\n",
    "\n",
    "arr = np.arange(144)  \n",
    "\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=RANDOM_STATE) # keep randomstate=100\n",
    "\n",
    "fold_subsets = []\n",
    "\n",
    "for train_index, val_index in kf.split(arr):\n",
    "    train_subset = arr[train_index]\n",
    "    test_subset = arr[val_index]\n",
    "    fold_subsets.append((train_subset, test_subset))\n",
    "    \n",
    "def get_training_ids(fold_subset):\n",
    "    return fold_subset[0]\n",
    "\n",
    "def get_validation_ids(fold_subset):\n",
    "    return fold_subset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example, print get val/train ids for i=0\n",
    "fold_subset = fold_subsets[0]\n",
    "training_ids = get_training_ids(fold_subset) # get validation ids\n",
    "validation_ids = get_validation_ids(fold_subset) # get validation ids\n",
    "print(f'fold {0}, train len:{len(training_ids)}, val len:{len(validation_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac2c30-7cba-4890-a9e5-2f123806c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip training folder. we do not create a validation folder but \n",
    "# load by the validation_ids from the k_fold\n",
    "\n",
    "if not os.path.isdir('training'): \n",
    "  try:\n",
    "    !unzip -o ethz-cil-road-segmentation-2023.zip\n",
    "    !mv training/training/* training\n",
    "    !rm -rf training/traininga\n",
    "  except:\n",
    "    print('Please upload a .zip file containing your datasets.')\n",
    "else:\n",
    "  print(\"Training folder exists already\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e641b2-ecfc-4858-b93e-e143927db1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('inpainted'):  # make sure this has not been executed yet\n",
    "  try:\n",
    "          !unzip -o inpainted.zip\n",
    "  except:\n",
    "      print('Please upload a .zip file containing the inpainted images.')\n",
    "else:\n",
    "    print(\"Inpainted images folder exists already\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44315920-3460-4241-ba79-29c3a8ab052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir('inpainted_images'): # rename folder to avoid problems when \"images\" is replaced in path\n",
    "    !mv inpainted_images inpainted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b445567-95c8-4253-8046-dc7400ce12c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('masked'):  # make sure this has not been executed yet\n",
    "  try:\n",
    "          !unzip -o masked.zip\n",
    "  except:\n",
    "      print('Please upload a .zip file containing the inpainted images.')\n",
    "else:\n",
    "    print(\"Masked images folder exists already\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839fc41-2ea8-43f5-8971-031c44510b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_from_path_by_id(path, idx):\n",
    "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
    "    # images are loaded as floats with values in the interval [0., 1.]\n",
    "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png')) if get_img_idx(f) == idx]).astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b6b33-2205-430c-8cd8-db2d02144146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_from_path_by_ids(path, ids):\n",
    "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
    "    # images are loaded as floats with values in the interval [0., 1.]\n",
    "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png')) if get_img_idx(f) in ids]).astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59180c-4651-4be9-8c91-f7e562f7ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_idx(path):\n",
    "    return int(re.search(r'\\d+', os.path.basename(path)).group()) # first number in filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667341b-cfd0-4106-b67b-a4ffbf2b0ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_from_path_except(path, exceptions=[]):\n",
    "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
    "    # images are loaded as floats with values in the interval [0., 1.]\n",
    "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png')) if get_img_idx(f) not in exceptions]).astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc35b8-9f00-4ba7-a084-307dfcb2ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_from_path(path):\n",
    "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
    "    # images are loaded as floats with values in the interval [0., 1.]\n",
    "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png'))]).astype(np.float32) / 255.\n",
    "\n",
    "\n",
    "def show_first_n(imgs, masks, n=5):\n",
    "    # visualizes the first n elements of a series of images and segmentation masks\n",
    "    imgs_to_draw = min(n, len(imgs))\n",
    "    fig, axs = plt.subplots(2, imgs_to_draw, figsize=(18.5, 6))\n",
    "    for i in range(imgs_to_draw):\n",
    "        axs[0, i].imshow(imgs[i])\n",
    "        axs[1, i].imshow(masks[i])\n",
    "        axs[0, i].set_title(f'Image {i}')\n",
    "        axs[1, i].set_title(f'Mask {i}')\n",
    "        axs[0, i].set_axis_off()\n",
    "        axs[1, i].set_axis_off()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# paths to training and validation datasets\n",
    "fold_i = 0\n",
    "training_ids = get_training_ids(fold_subsets[fold_i])\n",
    "val_ids = get_validation_ids(fold_subsets[fold_i])\n",
    "\n",
    "train_path = 'training'\n",
    "#val_path = 'validation'\n",
    "\n",
    "print(os.path.join(train_path, 'images'))\n",
    "train_images = load_all_from_path_by_ids(os.path.join(train_path, 'images'), training_ids)\n",
    "train_masks = load_all_from_path_by_ids(os.path.join(train_path, 'groundtruth'), training_ids)\n",
    "val_images = load_all_from_path_by_ids(os.path.join(train_path, 'images'), val_ids)\n",
    "val_masks = load_all_from_path_by_ids(os.path.join(train_path, 'groundtruth'), val_ids)\n",
    "\n",
    "# visualize a few images from the training set\n",
    "show_first_n(train_images, train_masks, n=2)\n",
    "print(f\"Training samples {len(train_images)}, Shape: {train_images.shape} \")\n",
    "# visualize a few images from the validation set\n",
    "show_first_n(val_images, val_masks, n=2)\n",
    "print(f\"Val. samples {len(val_images)}, Shape: {train_images.shape} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3f4e4-2bdd-4b8e-a97e-90d55f781748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE AUGMENTATION FOLDER. SET DELETE_AUG = True if you really want to remove it\n",
    "DELETE_AUG = False\n",
    "\n",
    "if DELETE_AUG:\n",
    "    !rm -r augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013fd31-fe03-449a-866f-d63dec0a242a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --user albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3e5ee-5b54-408a-9c85-1eb33b1d04f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.6),\n",
    "    A.RandomRotate90(p=0.6),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.ElasticTransform(p=0.5, alpha=5, sigma=10, alpha_affine=0)\n",
    "    #A.CoarseDropout(max_holes=10, max_height=40, max_width=100)\n",
    "])\n",
    "\n",
    "# create augmentation folder for each training_i folder\n",
    "for i in range(0, K):\n",
    "    src_images = f\"training/images/*.png\"\n",
    "    src_images_inpainted = \"inpainted/images/*.png\"\n",
    "    # src_images = \"masked/images/*.png\"\n",
    "\n",
    "    basic_augmentation_factor = 3\n",
    "    \n",
    "    inpaiting_augmentation_factor = 4\n",
    "    \n",
    "\n",
    "    dest_folder_img = f'augmentation/images'\n",
    "    dest_folder_mask = f'augmentation/groundtruth'\n",
    "\n",
    "    #val_indices = [get_img_idx(f) for f in glob(val_images_path)]\n",
    "\n",
    "    if not os.path.isdir('augmentation'):  # make sure this has not been executed yet\n",
    "        !mkdir augmentation\n",
    "        !mkdir augmentation/images\n",
    "        !mkdir augmentation/groundtruth\n",
    "\n",
    "        for img_path in glob(src_images):\n",
    "            # don't use variations of images in the validation set in case of using masked or inpainted images\n",
    "            #if get_img_idx(img_path) in val_indices:\n",
    "                #continue\n",
    "            # add basic augmentation\n",
    "            for i in range(basic_augmentation_factor):\n",
    "                image = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGBA)\n",
    "\n",
    "                mask_path = img_path.replace('images', 'groundtruth')\n",
    "                mask_path = mask_path.replace('_inpainted', '')\n",
    "                mask_path = mask_path.replace('_masked', '')\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "                transformed = transform(image=image, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "\n",
    "                file_name = os.path.basename(img_path)\n",
    "                image_name, image_ext = os.path.splitext(file_name) # get iamge name and extension\n",
    "                img_dest_path = os.path.join(dest_folder_img, f\"{image_name}_transformed_{i}{image_ext}\")\n",
    "                mask_dest_path = os.path.join(img_dest_path.replace(\"images\", \"groundtruth\"))\n",
    "\n",
    "                transformed_image = Image.fromarray(transformed_image)\n",
    "                transformed_image.save(img_dest_path)\n",
    "\n",
    "                transformed_mask = Image.fromarray(transformed_mask)\n",
    "                transformed_mask.save(mask_dest_path)\n",
    "                \n",
    "            # add each inpainted images with transformation\n",
    "            idx = get_img_idx(img_path) # get id of current source image\n",
    "            \n",
    "            images_inpainted_path = os.path.join(f'inpainted', 'images/*.png')\n",
    "            # get all inpainted file by id\n",
    "            inpainted_files = [f for f in sorted(glob(images_inpainted_path)) if get_img_idx(f) == idx]\n",
    "            \n",
    "            cnt = 0\n",
    "            for inpainted_file in inpainted_files:\n",
    "                if cnt >= inpaiting_augmentation_factor: # stop if we have reached augmentation fac\n",
    "                    break\n",
    "                \n",
    "                image = cv2.imread(inpainted_file, cv2.IMREAD_UNCHANGED)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGBA)\n",
    "\n",
    "                \n",
    "                # THE inpainted image has the same groundtruth as the source image!\n",
    "                mask_path = inpainted_file.replace('/images/', '/groundtruth/').replace('satimage_inpainted', 'satimage')\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "                transformed = transform(image=image, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "\n",
    "                file_name = os.path.basename(inpainted_file)\n",
    "                image_name, image_ext = os.path.splitext(file_name) # get image name and extension\n",
    "                img_dest_path = os.path.join(dest_folder_img, f\"{image_name}_transformed{image_ext}\")\n",
    "                mask_dest_path = os.path.join(img_dest_path.replace(\"images\", \"groundtruth\"))\n",
    "\n",
    "                transformed_image = Image.fromarray(transformed_image)\n",
    "                transformed_image.save(img_dest_path)\n",
    "\n",
    "                transformed_mask = Image.fromarray(transformed_mask)\n",
    "                transformed_mask.save(mask_dest_path)\n",
    "                \n",
    "                cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f3e64-96af-4a83-86fc-6f3cdc5aafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show augmentation sample for image 0\n",
    "aug_path = 'augmentation'\n",
    "idx = 0\n",
    "\n",
    "original_image = [train_images[idx]]\n",
    "original_mask = [train_masks[idx]]\n",
    "\n",
    "aug_images = load_all_from_path_by_id(os.path.join(aug_path, 'images'), idx)\n",
    "aug_masks = load_all_from_path_by_id(os.path.join(aug_path, 'groundtruth'), idx)\n",
    "\n",
    "disp_imgs = np.concatenate((original_image, aug_images), axis=0)\n",
    "disp_masks = np.concatenate((original_mask, aug_masks), axis=0)\n",
    "\n",
    "len(disp_imgs)\n",
    "# visualize a few images from the training set\n",
    "show_first_n(disp_imgs, disp_masks, 10)\n",
    "\n",
    "file_names = ['original']\n",
    "file_names.extend([f.split('/')[-1] for f in sorted(glob(os.path.join(aug_path, 'images') + '/*.png')) if get_img_idx(f) == idx])\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806c676-8fbb-4605-a283-c7561e4f8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def np_to_tensor(x, device):\n",
    "    # allocates tensors from np.arrays\n",
    "    if device == 'cpu':\n",
    "        return torch.from_numpy(x).cpu()\n",
    "    else:\n",
    "        return torch.from_numpy(x).contiguous().pin_memory().to(device=device, non_blocking=True)\n",
    "\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    # dataset class that deals with loading the data and making it available by index.\n",
    "\n",
    "    def __init__(self, path, device, sample_ids=[], use_patches=True, use_augmentation=False, use_inpainted_images=False, resize_to=(400, 400)):\n",
    "        self.path = path\n",
    "        self.device = device\n",
    "        self.sample_ids = sample_ids # shouldn't be empty!\n",
    "        self.use_patches = use_patches\n",
    "        self.use_augmentation = use_augmentation\n",
    "        self.use_inpainted_images = use_inpainted_images\n",
    "        self.resize_to=resize_to\n",
    "        self.x, self.y, self.n_samples = None, None, None\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):  # not very scalable, but good enough for now\n",
    "        sample_ids = self.sample_ids\n",
    "        self.x = load_all_from_path_by_ids(os.path.join(self.path, 'images'), sample_ids)[:,:,:,:3]\n",
    "        self.y = load_all_from_path_by_ids(os.path.join(self.path, 'groundtruth'), sample_ids)\n",
    "        \n",
    "        if self.use_augmentation:\n",
    "             # load from augmentation path and concat x and y\n",
    "            aug_path = f'augmentation'\n",
    "            aug_x = load_all_from_path_by_ids(os.path.join(aug_path, 'images'), sample_ids)[:,:,:,:3]\n",
    "            aug_y = load_all_from_path_by_ids(os.path.join(aug_path, 'groundtruth'), sample_ids)\n",
    "            \n",
    "            self.x = np.concatenate((self.x, aug_x), axis=0)\n",
    "            self.y = np.concatenate((self.y, aug_y), axis=0)\n",
    "            \n",
    "        if self.use_inpainted_images:\n",
    "            inpainted_path = 'inpainted'\n",
    "            \n",
    "            #val_images_path = os.path.join(f'validation', 'images', '*.png')\n",
    "            #val_indices = [get_img_idx(f) for f in glob(val_images_path)]\n",
    "            \n",
    "            # TODO check with gian quickly\n",
    "            \n",
    "            inpainted_x = load_all_from_path_by_ids(os.path.join(inpainted_path, 'images'), sample_ids)[:,:,:,:3]\n",
    "            inpainted_y = load_all_from_path_by_ids(os.path.join(inpainted_path, 'groundtruth'), sample_ids)\n",
    "            \n",
    "            self.x = np.concatenate((self.x, inpainted_x), axis=0)\n",
    "            self.y = np.concatenate((self.y, inpainted_y), axis=0)\n",
    "       \n",
    "        if self.use_patches:  # split each image into patches\n",
    "            self.x, self.y = image_to_patches(self.x, self.y)\n",
    "        elif self.resize_to != (self.x.shape[1], self.x.shape[2]):  # resize images\n",
    "            self.x = np.stack([cv2.resize(img, dsize=self.resize_to) for img in self.x], 0)\n",
    "            self.y = np.stack([cv2.resize(mask, dsize=self.resize_to) for mask in self.y], 0)\n",
    "        self.x = np.moveaxis(self.x, -1, 1)  # pytorch works with CHW format instead of HWC\n",
    "        \n",
    "        \n",
    "        self.n_samples = len(self.x)\n",
    "\n",
    "    def _preprocess(self, x, y):\n",
    "        # to keep things simple we will not apply transformations to each sample,\n",
    "        # but it would be a very good idea to look into preprocessing\n",
    "        return x, y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._preprocess(np_to_tensor(self.x[item], self.device), np_to_tensor(self.y[[item]], self.device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "def show_val_samples(x, y, y_hat, segmentation=False):\n",
    "    # training callback to show predictions on validation set\n",
    "    imgs_to_draw = min(5, len(x))\n",
    "    if x.shape[-2:] == y.shape[-2:]:  # segmentation\n",
    "        fig, axs = plt.subplots(3, imgs_to_draw, figsize=(18.5, 12))\n",
    "        for i in range(imgs_to_draw):\n",
    "            axs[0, i].imshow(np.moveaxis(x[i], 0, -1))\n",
    "            axs[1, i].imshow(np.concatenate([np.moveaxis(y_hat[i], 0, -1)] * 3, -1))\n",
    "            axs[2, i].imshow(np.concatenate([np.moveaxis(y[i], 0, -1)]*3, -1))\n",
    "            axs[0, i].set_title(f'Sample {i}')\n",
    "            axs[1, i].set_title(f'Predicted {i}')\n",
    "            axs[2, i].set_title(f'True {i}')\n",
    "            axs[0, i].set_axis_off()\n",
    "            axs[1, i].set_axis_off()\n",
    "            axs[2, i].set_axis_off()\n",
    "    else:  # classification\n",
    "        fig, axs = plt.subplots(1, imgs_to_draw, figsize=(18.5, 6))\n",
    "        for i in range(imgs_to_draw):\n",
    "            axs[i].imshow(np.moveaxis(x[i], 0, -1))\n",
    "            axs[i].set_title(f'True: {np.round(y[i]).item()}; Predicted: {np.round(y_hat[i]).item()}')\n",
    "            axs[i].set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cabbec-6107-4f28-ac14-d6e2796b8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of getting a dataset based on sample ids\n",
    "fold_i = 0\n",
    "training_ids = get_training_ids(fold_subsets[fold_i])\n",
    "val_ids = get_validation_ids(fold_subsets[fold_i])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_dataset = ImageDataset('training', device, sample_ids=training_ids, use_patches=False, use_augmentation=False, resize_to=(384, 384))\n",
    "\n",
    "print(train_dataset.n_samples) # standard dataset has 134\n",
    "train_dataset._load_data()\n",
    "train_dataset = ImageDataset('training', device, sample_ids=training_ids, use_patches=False, use_augmentation=True, use_inpainted_images=False, resize_to=(384, 384))\n",
    "print(train_dataset.n_samples) # check if we have more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b4e2b-e292-46ee-b315-7f5bea225e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, eval_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs, model_filename):\n",
    "    # training loop\n",
    "    logdir = './tensorboard/net'\n",
    "    writer = SummaryWriter(logdir)  # tensorboard writer (can also log images)\n",
    "\n",
    "    history = {}  # collects metrics at the end of each epoch\n",
    "\n",
    "        \n",
    "    best_val_patch_acc = 0.0\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        # initialize metric list\n",
    "        metrics = {'loss': [], 'val_loss': []}\n",
    "        for k, _ in metric_fns.items():\n",
    "            metrics[k] = []\n",
    "            metrics['val_'+k] = []\n",
    "\n",
    "        pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}')\n",
    "        # training\n",
    "        model.train()\n",
    "        for (x, y) in pbar:\n",
    "            optimizer.zero_grad()  # zero out gradients\n",
    "            y_hat = model(x)  # forward pass\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()  # backward pass\n",
    "            optimizer.step()  # optimize weights\n",
    "\n",
    "            # log partial metrics\n",
    "            metrics['loss'].append(loss.item())\n",
    "            for k, fn in metric_fns.items():\n",
    "                metrics[k].append(fn(y_hat, y).item())\n",
    "            pbar.set_postfix({k: sum(v)/len(v) for k, v in metrics.items() if len(v) > 0})\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        (show_x, show_y, show_y_hat) = (None, None, None) # variable that saves the last validation batch > 0\n",
    "        with torch.no_grad():  # do not keep track of gradients\n",
    "            for (x, y) in eval_dataloader:\n",
    "                y_hat = model(x)  # forward pass\n",
    "                loss = loss_fn(y_hat, y)\n",
    "\n",
    "                # log partial metrics\n",
    "                metrics['val_loss'].append(loss.item())\n",
    "                for k, fn in metric_fns.items():\n",
    "                    metrics['val_'+k].append(fn(y_hat, y).item())\n",
    "                    \n",
    "                # if x,y,y_hat is of a batch > 1, then we present it to show_val_samples\n",
    "                if len(x) > 1:\n",
    "                    (show_x, show_y, show_y_hat) = (x,y,y_hat)\n",
    "\n",
    "        # summarize metrics, log to tensorboard and display\n",
    "        history[epoch] = {k: sum(v) / len(v) for k, v in metrics.items()}\n",
    "        for k, v in history[epoch].items():\n",
    "          writer.add_scalar(k, v, epoch)\n",
    "        print(' '.join(['\\t- '+str(k)+' = '+str(v)+'\\n ' for (k, v) in history[epoch].items()]))\n",
    "        \n",
    "        if (epoch +1) % 10 == 0: # only show after 10 epochs\n",
    "            show_val_samples(show_x.detach().cpu().numpy(), show_y.detach().cpu().numpy(), show_y_hat.detach().cpu().numpy())\n",
    "        \n",
    "        current_val_acc = np.mean(metrics['val_patch_acc']) # i think we need to get the mean here\n",
    "        \n",
    "        if best_val_patch_acc < current_val_acc:\n",
    "            print(f'found best patch_acc: {current_val_acc}')\n",
    "            best_val_patch_acc = current_val_acc\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "\n",
    "    print('Finished Training')\n",
    "    # plot loss curves\n",
    "    plt.plot([v['loss'] for k, v in history.items()], label='Training Loss')\n",
    "    plt.plot([v['val_loss'] for k, v in history.items()], label='Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # return best val. acc\n",
    "    print(f'Best validation patch accuracy: {best_val_patch_acc}')\n",
    "    return best_val_patch_acc\n",
    "\n",
    "    #torch.save(model.state_dict(), 'base_unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37583c9-aaac-4f3e-8d6f-ad88df01d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # a repeating structure composed of two convolutional layers with batch normalization and ReLU activations\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.BatchNorm2d(out_ch),\n",
    "                                   nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
    "                                   nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    # UNet-like architecture for single class semantic segmentation.\n",
    "    def __init__(self, chs=(3,64,128,256,512,1024), p_dropout=0.0):\n",
    "        super().__init__()\n",
    "        enc_chs = chs  # number of channels in the encoder\n",
    "        dec_chs = chs[::-1][:-1]  # number of channels in the decoder # [::-1] reverse, [:-1] select all until last\n",
    "        self.enc_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
    "        self.pool = nn.MaxPool2d(2)  # pooling layer (can be reused as it will not be trained)\n",
    "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_ch, out_ch, 2, 2) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # deconvolution\n",
    "        self.dec_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # decoder blocks\n",
    "        self.head = nn.Sequential(nn.Conv2d(dec_chs[-1], 1, 1), nn.Sigmoid()) # 1x1 convolution for producing the output\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        enc_features = []\n",
    "        for block in self.enc_blocks[:-1]:\n",
    "            x = block(x)  # pass through the block\n",
    "            x = self.dropout(x)\n",
    "            enc_features.append(x)  # save features for skip connections\n",
    "            x = self.pool(x)  # decrease resolution\n",
    "        x = self.enc_blocks[-1](x)\n",
    "        # decode\n",
    "        for block, upconv, feature in zip(self.dec_blocks, self.upconvs, enc_features[::-1]):\n",
    "            x = upconv(x)  # increase resolution\n",
    "            x = torch.cat([x, feature], dim=1)  # concatenate skip features\n",
    "            x = block(x)  # pass through the block\n",
    "        return self.head(x)  # reduce to 1 channel\n",
    "\n",
    "\n",
    "def patch_accuracy_fn(y_hat, y):\n",
    "    # computes accuracy weighted by patches (metric used on Kaggle for evaluation)\n",
    "    h_patches = y.shape[-2] // PATCH_SIZE\n",
    "    w_patches = y.shape[-1] // PATCH_SIZE\n",
    "    patches_hat = y_hat.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
    "    patches = y.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
    "    return (patches == patches_hat).float().mean()\n",
    "\n",
    "def dice_similarity_fn(y_hat, y):\n",
    "    # computes dice similarity\n",
    "    overlap = (y_hat.round() == y.round()).float().sum()\n",
    "    return 2*overlap  / (2*overlap + y_hat.round().sum() + y.round().sum())\n",
    "\n",
    "def accuracy_fn(y_hat, y):\n",
    "    # computes classification accuracy\n",
    "    return (y_hat.round() == y.round()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27325dfb-276c-4375-b626-691af2dfd4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experiment loop\n",
    "import pandas as pd\n",
    "val_patch_acc = []\n",
    "experiment = 'inpainted' # this is used to save the trained models and also csv that contains the\n",
    "# validation accuracies\n",
    "epochs = 35\n",
    "\n",
    "for fold_i in range(0,K):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Fold {fold_i}: Training using {device}\")\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "    # get sample ids for validation and training of our k-fld\n",
    "    training_ids = get_training_ids(fold_subsets[fold_i])\n",
    "    val_ids = get_validation_ids(fold_subsets[fold_i])\n",
    "    \n",
    "    # SET augmentation=True if you wan't to use inpaitings etc.\n",
    "    train_dataset = ImageDataset('training', device, sample_ids=training_ids, use_patches=False, use_augmentation=True, resize_to=(384, 384))\n",
    "    \n",
    "    # make sure val_dataset gets the val_ids!\n",
    "    val_dataset = ImageDataset('training', device, sample_ids=val_ids, use_patches=False, use_augmentation=False, resize_to=(384, 384))\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "    model = UNet().to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn}\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # train on full dataset\n",
    "    best_val_patch_acc = train(train_dataloader, val_dataloader, model, loss_fn, metric_fns, optimizer, epochs, model_filename=f'{experiment}_fold_{fold_i}')\n",
    "    val_patch_acc.append(best_val_patch_acc)\n",
    "    \n",
    "# save results in csv\n",
    "df = pd.DataFrame(val_patch_acc, columns=['val_patch_acc'])\n",
    "df.to_csv(f\"{experiment}_val_acc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879516c-edf7-4d10-b560-b78ae145fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(f\"{experiment}_val_acc.csv\")\n",
    "\n",
    "arr = df.to_numpy()\n",
    "\n",
    "mean = np.mean(arr)\n",
    "var = np.var(arr)\n",
    "max_id = np.argmax(arr)\n",
    "max_val_patch_acc = arr[max_id][-1]\n",
    "\n",
    "print(f'mean: {mean}, var: {var}, best_fold: {max_id}, with acc: {max_val_patch_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c6747-a7bf-4ddf-a787-684cfbe7cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SE = np.sqrt(var)/np.sqrt(K)\n",
    "print(f'SE: {SE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e7e63-4456-422e-9db7-b728cda44abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_submission(labels, test_filenames, submission_filename):\n",
    "    test_path='test/images'\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for fn, patch_array in zip(sorted(test_filenames), test_pred):\n",
    "            img_number = int(re.search(r\"\\d+\", fn).group(0))\n",
    "            for i in range(patch_array.shape[0]):\n",
    "                for j in range(patch_array.shape[1]):\n",
    "                    f.write(\"{:03d}_{}_{},{}\\n\".format(img_number, j*PATCH_SIZE, i*PATCH_SIZE, int(patch_array[i, j])))\n",
    "\n",
    "test_path = 'test/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf56a54-0320-4416-9a3f-0a1e802db4c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "device = 'cuda'\n",
    "model = UNet().to(device)\n",
    "best_fold_name = 'inpainted_fold_5' # use your best model based on val acc in csv, manually change it to make a submission\n",
    "model.load_state_dict(torch.load(best_fold_name)) \n",
    "\n",
    "\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0] # number of test images\n",
    "size = test_images.shape[1:3] #WH of images (currently its 400x400 -> reshape to 384x384 for model)\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(400, 400)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3] # only get first three channels\n",
    "test_images = np_to_tensor(np.moveaxis(test_images, -1, 1), device) # switch from HWC to CWH\n",
    "test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)] # use our model to predict segmentation mask\n",
    "test_pred = np.concatenate(test_pred, 0)\n",
    "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
    "\n",
    "# now test_pred has shape (144, 400, 400), for each image the pixel have values 0-1.\n",
    "\n",
    "# now compute labels\n",
    "test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE)) # split in patches\n",
    "test_pred = np.moveaxis(test_pred, 2, 3) # move dimension 16x16 together\n",
    "test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF) # mean of 16x16, classify 0 or 1\n",
    "create_submission(test_pred, test_filenames, submission_filename=f'{best_fold_name}_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68009941-3d8b-44c8-bfe3-ac34b90787ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine (average) predicitions of the different folds\n",
    "device = 'cuda'\n",
    "model = UNet().to(device)\n",
    "\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0] # number of test images\n",
    "size = test_images.shape[1:3] #WH of images (currently its 400x400 -> reshape to 384x384 for model)\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(400, 400)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3] # only get first three channels\n",
    "test_images = np_to_tensor(np.moveaxis(test_images, -1, 1), device) # switch from HWC to CWH\n",
    "\n",
    "for i in range(K):\n",
    "    fold_name = f'inpainted_fold_{i}' # use your best model based on val acc in csv, manually change it to make a submission\n",
    "    model.load_state_dict(torch.load(fold_name))\n",
    "    \n",
    "    test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)] # use our model to predict segmentation mask\n",
    "    test_pred = np.concatenate(test_pred, 0)\n",
    "    test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "    test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
    "    \n",
    "    if i == 0:\n",
    "        total_test_pred = test_pred\n",
    "    else:\n",
    "        total_test_pred += test_pred\n",
    "\n",
    "avg_test_pred = total_test_pred / K\n",
    "\n",
    "# now compute labels\n",
    "test_pred = avg_test_pred\n",
    "test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE)) # split in patches\n",
    "test_pred = np.moveaxis(test_pred, 2, 3) # move dimension 16x16 together\n",
    "test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF) # mean of 16x16, classify 0 or 1\n",
    "create_submission(test_pred, test_filenames, submission_filename=f'averaged_submission.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83499e3c-ed47-4ef4-bc2e-2458d8f32290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
