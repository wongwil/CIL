{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12737a5f-7aaf-44d7-9025-52b7f35a0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from random import sample\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f0196-dd22-4bff-9775-4f8c4d093819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefined for the project as the task is considered as classification in patches\n",
    "# some constants\n",
    "PATCH_SIZE = 16  # pixels per side of square patches\n",
    "VAL_SIZE = 10  # size of the validation set (number of images)\n",
    "CUTOFF = 0.25  # minimum average brightness for a mask patch to be classified as containing road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wsl ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 5\n",
    "\n",
    "arr = np.arange(144)  \n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=100)\n",
    "\n",
    "fold_subsets = []\n",
    "\n",
    "for train_index, val_index in kf.split(arr):\n",
    "    train_subset = arr[train_index]\n",
    "    test_subset = arr[val_index]\n",
    "    fold_subsets.append((train_subset, test_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_ID = 1 # change it manually to which k-fold subset we are using currently [0,...,4]\n",
    "validation_ids = fold_subsets[SUBSET_ID][1] # get validation ids\n",
    "print(validation_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70124d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing folders and files to we can recreate the training and validation folder\n",
    "!rm -r validation\n",
    "!rm -r training\n",
    "!rm -r test\n",
    "!rm mask_to_submission.py\n",
    "!rm submission_to_mask.py \n",
    "\n",
    "# create training and validation folder according to the k-fold subset\n",
    "try:\n",
    "        !unzip ethz-cil-road-segmentation-2023.zip\n",
    "        !rm -rf training/training\n",
    "        !mkdir validation\n",
    "        !mkdir validation/images\n",
    "        !mkdir validation/groundtruth\n",
    "        for img in glob(\"training/images/*.png\"):\n",
    "            for val_id in validation_ids:\n",
    "                if img.endswith(f'satimage_{val_id}.png'):\n",
    "                    os.rename(img, img.replace('training', 'validation'))\n",
    "                    mask = img.replace('images', 'groundtruth') # replace path\n",
    "                    os.rename(mask, mask.replace('training', 'validation'))\n",
    "                    break\n",
    "except:\n",
    "    print('Please upload a .zip file containing your datasets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc9571-0db8-48ca-a903-3930263835bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('validation'):  # make sure this has not been executed yet\n",
    "  try:\n",
    "    !wsl unzip -o ethz-cil-road-segmentation-2023.zip\n",
    "    !wsl mv training/training/* training\n",
    "    !wsl rm -rf training/training\n",
    "    !wsl mkdir validation\n",
    "    !wsl mkdir validation/images\n",
    "    !wsl mkdir validation/groundtruth\n",
    "    for img in sample(glob(\"training/images/*.png\"), VAL_SIZE):\n",
    "      os.rename(img, img.replace('training', 'validation'))\n",
    "      mask = img.replace('images', 'groundtruth')\n",
    "      os.rename(mask, mask.replace('training', 'validation'))\n",
    "  except:\n",
    "    print('Please upload a .zip file containing your datasets.')\n",
    "else:\n",
    "  print(\"Validation folder exists already\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e641b2-ecfc-4858-b93e-e143927db1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('masked'):  # make sure this has not been executed yet\n",
    "  try:\n",
    "          !wsl unzip -o masked.zip\n",
    "  except:\n",
    "      print('Please upload a .zip file containing the inpainted images.')\n",
    "else:\n",
    "    print(\"Masked images folder exists already\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc35b8-9f00-4ba7-a084-307dfcb2ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_from_path(path):\n",
    "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
    "    # images are loaded as floats with values in the interval [0., 1.]\n",
    "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png'))]).astype(np.float32) / 255.\n",
    "\n",
    "\n",
    "def show_first_n(imgs, masks, n=5):\n",
    "    # visualizes the first n elements of a series of images and segmentation masks\n",
    "    imgs_to_draw = min(n, len(imgs))\n",
    "    fig, axs = plt.subplots(2, imgs_to_draw, figsize=(18.5, 6))\n",
    "    for i in range(imgs_to_draw):\n",
    "        axs[0, i].imshow(imgs[i])\n",
    "        axs[1, i].imshow(masks[i])\n",
    "        axs[0, i].set_title(f'Image {i}')\n",
    "        axs[1, i].set_title(f'Mask {i}')\n",
    "        axs[0, i].set_axis_off()\n",
    "        axs[1, i].set_axis_off()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# paths to training and validation datasets\n",
    "train_path = 'training'\n",
    "val_path = 'validation'\n",
    "\n",
    "print(os.path.join(train_path, 'images'))\n",
    "train_images = load_all_from_path(os.path.join(train_path, 'images'))\n",
    "train_masks = load_all_from_path(os.path.join(train_path, 'groundtruth'))\n",
    "val_images = load_all_from_path(os.path.join(val_path, 'images'))\n",
    "val_masks = load_all_from_path(os.path.join(val_path, 'groundtruth'))\n",
    "\n",
    "# visualize a few images from the training set\n",
    "show_first_n(train_images, train_masks, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa48982-142d-4d0a-9c9a-42535b58f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training samples {len(train_images)}, Validation. samples {len(val_images)}, Shape: {train_images.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839fc41-2ea8-43f5-8971-031c44510b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_from_path_by_id(path, idx):\n",
    "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
    "    # images are loaded as floats with values in the interval [0., 1.]\n",
    "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*_' + str(idx) + '*.png'))]).astype(np.float32) / 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59180c-4651-4be9-8c91-f7e562f7ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_idx(path):\n",
    "    return int(re.search(r'\\d+', os.path.basename(path)).group()) # first number in filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667341b-cfd0-4106-b67b-a4ffbf2b0ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_from_path_except(path, exceptions=[]):\n",
    "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
    "    # images are loaded as floats with values in the interval [0., 1.]\n",
    "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png')) if get_img_idx(f) not in exceptions]).astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3f4e4-2bdd-4b8e-a97e-90d55f781748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE AUGMENTATION FOLDER. SET DELETE_AUG = True if you really want to remove it\n",
    "DELETE_AUG = False\n",
    "\n",
    "if DELETE_AUG:\n",
    "    !rm -r augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013fd31-fe03-449a-866f-d63dec0a242a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --user albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3e5ee-5b54-408a-9c85-1eb33b1d04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.6),\n",
    "    A.RandomRotate90(p=0.6),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.ElasticTransform(p=0.5, alpha=5, sigma=10, alpha_affine=0),\n",
    "    # A.CoarseDropout(max_holes=10, max_height=40, max_width=100)\n",
    "    A.CoarseDropout(max_holes=10, max_height=40, max_width=100)\n",
    "])\n",
    "\n",
    "src_images = \"training/images/*.png\"\n",
    "# src_images = \"inpainted/images/*.png\"\n",
    "# src_images = \"masked/images/*.png\"\n",
    "\n",
    "augmentation_factor = 4\n",
    "\n",
    "dest_folder_img = 'augmentation/images'\n",
    "dest_folder_mask = 'augmentation/groundtruth'\n",
    "\n",
    "val_images_path = os.path.join('validation', 'images', '*.png')\n",
    "val_indices = [get_img_idx(f) for f in glob(val_images_path)]\n",
    "\n",
    "if not os.path.isdir('augmentation'):  # make sure this has not been executed yet\n",
    "    !wsl mkdir augmentation\n",
    "    !wsl mkdir augmentation/images\n",
    "    !wsl mkdir augmentation/groundtruth\n",
    "    \n",
    "    for img_path in glob(src_images):\n",
    "        # don't use variations of images in the validation set in case of using masked or inpainted images\n",
    "        if get_img_idx(img_path) in val_indices:\n",
    "            continue\n",
    "            \n",
    "        for i in range(augmentation_factor):\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGBA)\n",
    "\n",
    "            mask_path = img_path.replace('images', 'groundtruth')\n",
    "            # mask_path = mask_path.replace('_inpainted', '')\n",
    "            # mask_path = mask_path.replace('_masked', '')\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            transformed = transform(image=image, mask=mask)\n",
    "            transformed_image = transformed['image']\n",
    "            transformed_mask = transformed['mask']\n",
    "\n",
    "            file_name = os.path.basename(img_path)\n",
    "            image_name, image_ext = os.path.splitext(file_name) # get iamge name and extension\n",
    "            img_dest_path = os.path.join(dest_folder_img, f\"{image_name}_transformed_{i}{image_ext}\")\n",
    "            mask_dest_path = os.path.join(img_dest_path.replace(\"images\", \"groundtruth\"))\n",
    "            \n",
    "            transformed_image = Image.fromarray(transformed_image)\n",
    "            transformed_image.save(img_dest_path)\n",
    "    \n",
    "            transformed_mask = Image.fromarray(transformed_mask)\n",
    "            transformed_mask.save(mask_dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f3e64-96af-4a83-86fc-6f3cdc5aafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample for image 0\n",
    "aug_path = 'augmentation'\n",
    "idx = 0\n",
    "\n",
    "original_image = [train_images[idx]]\n",
    "original_mask = [train_masks[idx]]\n",
    "\n",
    "aug_images = load_all_from_path_by_id(os.path.join(aug_path, 'images'), idx)\n",
    "aug_masks = load_all_from_path_by_id(os.path.join(aug_path, 'groundtruth'), idx)\n",
    "\n",
    "\n",
    "disp_imgs = np.concatenate((original_image, aug_images), axis=0)\n",
    "disp_masks = np.concatenate((original_mask, aug_masks), axis=0)\n",
    "\n",
    "len(disp_imgs)\n",
    "# visualize a few images from the training set\n",
    "show_first_n(disp_imgs, disp_masks, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806c676-8fbb-4605-a283-c7561e4f8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def np_to_tensor(x, device):\n",
    "    # allocates tensors from np.arrays\n",
    "    if device == 'cpu':\n",
    "        return torch.from_numpy(x).cpu()\n",
    "    else:\n",
    "        return torch.from_numpy(x).contiguous().pin_memory().to(device=device, non_blocking=True)\n",
    "\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    # dataset class that deals with loading the data and making it available by index.\n",
    "\n",
    "    def __init__(self, path, device, use_patches=True, use_augmentation=False, use_inpainted_images=False, resize_to=(400, 400)):\n",
    "        self.path = path\n",
    "        self.device = device\n",
    "        self.use_patches = use_patches\n",
    "        self.use_augmentation = use_augmentation\n",
    "        self.use_inpainted_images = use_inpainted_images\n",
    "        self.resize_to=resize_to\n",
    "        self.x, self.y, self.n_samples = None, None, None\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):  # not very scalable, but good enough for now\n",
    "        self.x = load_all_from_path(os.path.join(self.path, 'images'))[:,:,:,:3]\n",
    "        self.y = load_all_from_path(os.path.join(self.path, 'groundtruth'))\n",
    "        \n",
    "        if self.use_augmentation:\n",
    "             # load from augmentation path and concat x and y\n",
    "            aug_path = 'augmentation'\n",
    "            aug_x = load_all_from_path(os.path.join(aug_path, 'images'))[:,:,:,:3]\n",
    "            aug_y = load_all_from_path(os.path.join(aug_path, 'groundtruth'))\n",
    "            \n",
    "            self.x = np.concatenate((self.x, aug_x), axis=0)\n",
    "            self.y = np.concatenate((self.y, aug_y), axis=0)\n",
    "            \n",
    "        if self.use_inpainted_images:\n",
    "            inpainted_path = 'inpainted'\n",
    "            \n",
    "            val_images_path = os.path.join('validation', 'images', '*.png')\n",
    "            val_indices = [get_img_idx(f) for f in glob(val_images_path)]\n",
    "            \n",
    "            inpainted_x = load_all_from_path_except(os.path.join(inpainted_path, 'images'), exceptions=val_indices)[:,:,:,:3]\n",
    "            inpainted_y = load_all_from_path_except(os.path.join(inpainted_path, 'groundtruth'), exceptions=val_indices)\n",
    "            \n",
    "            self.x = np.concatenate((self.x, inpainted_x), axis=0)\n",
    "            self.y = np.concatenate((self.y, inpainted_y), axis=0)\n",
    "       \n",
    "        if self.use_patches:  # split each image into patches\n",
    "            self.x, self.y = image_to_patches(self.x, self.y)\n",
    "        elif self.resize_to != (self.x.shape[1], self.x.shape[2]):  # resize images\n",
    "            self.x = np.stack([cv2.resize(img, dsize=self.resize_to) for img in self.x], 0)\n",
    "            self.y = np.stack([cv2.resize(mask, dsize=self.resize_to) for mask in self.y], 0)\n",
    "        self.x = np.moveaxis(self.x, -1, 1)  # pytorch works with CHW format instead of HWC\n",
    "        \n",
    "        \n",
    "        self.n_samples = len(self.x)\n",
    "\n",
    "    def _preprocess(self, x, y):\n",
    "        # to keep things simple we will not apply transformations to each sample,\n",
    "        # but it would be a very good idea to look into preprocessing\n",
    "        return x, y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._preprocess(np_to_tensor(self.x[item], self.device), np_to_tensor(self.y[[item]], self.device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "def show_val_samples(x, y, y_hat, segmentation=False):\n",
    "    # training callback to show predictions on validation set\n",
    "    imgs_to_draw = min(5, len(x))\n",
    "    if x.shape[-2:] == y.shape[-2:]:  # segmentation\n",
    "        fig, axs = plt.subplots(3, imgs_to_draw, figsize=(18.5, 12))\n",
    "        for i in range(imgs_to_draw):\n",
    "            axs[0, i].imshow(np.moveaxis(x[i], 0, -1))\n",
    "            axs[1, i].imshow(np.concatenate([np.moveaxis(y_hat[i], 0, -1)] * 3, -1))\n",
    "            axs[2, i].imshow(np.concatenate([np.moveaxis(y[i], 0, -1)]*3, -1))\n",
    "            axs[0, i].set_title(f'Sample {i}')\n",
    "            axs[1, i].set_title(f'Predicted {i}')\n",
    "            axs[2, i].set_title(f'True {i}')\n",
    "            axs[0, i].set_axis_off()\n",
    "            axs[1, i].set_axis_off()\n",
    "            axs[2, i].set_axis_off()\n",
    "    else:  # classification\n",
    "        fig, axs = plt.subplots(1, imgs_to_draw, figsize=(18.5, 6))\n",
    "        for i in range(imgs_to_draw):\n",
    "            axs[i].imshow(np.moveaxis(x[i], 0, -1))\n",
    "            axs[i].set_title(f'True: {np.round(y[i]).item()}; Predicted: {np.round(y_hat[i]).item()}')\n",
    "            axs[i].set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa8c93-be77-4f94-a6d7-87f10b97a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training using {device}\")\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset('training', device, use_patches=False, resize_to=(384, 384))\n",
    "len(train_dataset.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cabbec-6107-4f28-ac14-d6e2796b8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_dataset = ImageDataset('training', device, use_patches=False, use_augmentation=False, resize_to=(384, 384))\n",
    "\n",
    "print(train_dataset.n_samples) # standard dataset has 134\n",
    "train_dataset._load_data()\n",
    "train_dataset = ImageDataset('training', device, use_patches=False, use_augmentation=True, use_inpainted_images=False, resize_to=(384, 384))\n",
    "print(train_dataset.n_samples) # check if we have more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b4e2b-e292-46ee-b315-7f5bea225e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, eval_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs):\n",
    "    # training loop\n",
    "    logdir = './tensorboard/net'\n",
    "    writer = SummaryWriter(logdir)  # tensorboard writer (can also log images)\n",
    "\n",
    "    history = {}  # collects metrics at the end of each epoch\n",
    "    \n",
    "    # best_val_patch_acc = 0\n",
    "    best_val_loss = 1\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        # initialize metric list\n",
    "        metrics = {'loss': [], 'val_loss': []}\n",
    "        for k, _ in metric_fns.items():\n",
    "            metrics[k] = []\n",
    "            metrics['val_'+k] = []\n",
    "\n",
    "        pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}')\n",
    "        # training\n",
    "        model.train()\n",
    "        for (x, y) in pbar:\n",
    "            optimizer.zero_grad()  # zero out gradients\n",
    "            y_hat = model(x)  # forward pass\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()  # backward pass\n",
    "            optimizer.step()  # optimize weights\n",
    "\n",
    "            # log partial metrics\n",
    "            metrics['loss'].append(loss.item())\n",
    "            for k, fn in metric_fns.items():\n",
    "                metrics[k].append(fn(y_hat, y).item())\n",
    "            pbar.set_postfix({k: sum(v)/len(v) for k, v in metrics.items() if len(v) > 0})\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # do not keep track of gradients\n",
    "            for (x, y) in eval_dataloader:\n",
    "                y_hat = model(x)  # forward pass\n",
    "                loss = loss_fn(y_hat, y)\n",
    "\n",
    "                # log partial metrics\n",
    "                metrics['val_loss'].append(loss.item())\n",
    "                for k, fn in metric_fns.items():\n",
    "                    metrics['val_'+k].append(fn(y_hat, y).item())\n",
    "\n",
    "        # summarize metrics, log to tensorboard and display\n",
    "        history[epoch] = {k: sum(v) / len(v) for k, v in metrics.items()}\n",
    "        for k, v in history[epoch].items():\n",
    "          writer.add_scalar(k, v, epoch)\n",
    "        print(' '.join(['\\t- '+str(k)+' = '+str(v)+'\\n ' for (k, v) in history[epoch].items()]))\n",
    "        \n",
    "        if epoch % 2 == 0: # only show after 5 epochs\n",
    "            show_val_samples(x.detach().cpu().numpy(), y.detach().cpu().numpy(), y_hat.detach().cpu().numpy())\n",
    "            \n",
    "        # if metrics['val_patch_acc'][-1] > best_val_patch_acc:\n",
    "        #     best_val_patch_acc = metrics['val_patch_acc'][-1]\n",
    "        #     print('saving...')\n",
    "        #     torch.save(model.state_dict(), 'unet')\n",
    "        \n",
    "        if metrics['val_loss'][-1] < best_val_loss:\n",
    "            best_val_loss = metrics['val_loss'][-1]\n",
    "            print('saving...')\n",
    "            torch.save(model.state_dict(), 'unet')\n",
    "\n",
    "    print('Finished Training')\n",
    "    # plot loss curves\n",
    "    plt.plot([v['loss'] for k, v in history.items()], label='Training Loss')\n",
    "    plt.plot([v['val_loss'] for k, v in history.items()], label='Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37583c9-aaac-4f3e-8d6f-ad88df01d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # a repeating structure composed of two convolutional layers with batch normalization and ReLU activations\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.BatchNorm2d(out_ch),\n",
    "                                   nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
    "                                   nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    # UNet-like architecture for single class semantic segmentation.\n",
    "    def __init__(self, chs=(3,64,128,256,512,1024), p_dropout=0.0):\n",
    "        super().__init__()\n",
    "        enc_chs = chs  # number of channels in the encoder\n",
    "        dec_chs = chs[::-1][:-1]  # number of channels in the decoder # [::-1] reverse, [:-1] select all until last\n",
    "        self.enc_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
    "        self.pool = nn.MaxPool2d(2)  # pooling layer (can be reused as it will not be trained)\n",
    "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_ch, out_ch, 2, 2) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # deconvolution\n",
    "        self.dec_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # decoder blocks\n",
    "        self.head = nn.Sequential(nn.Conv2d(dec_chs[-1], 1, 1), nn.Sigmoid()) # 1x1 convolution for producing the output\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        enc_features = []\n",
    "        for block in self.enc_blocks[:-1]:\n",
    "            x = block(x)  # pass through the block\n",
    "            x = self.dropout(x)\n",
    "            enc_features.append(x)  # save features for skip connections\n",
    "            x = self.pool(x)  # decrease resolution\n",
    "        x = self.enc_blocks[-1](x)\n",
    "        # decode\n",
    "        for block, upconv, feature in zip(self.dec_blocks, self.upconvs, enc_features[::-1]):\n",
    "            x = upconv(x)  # increase resolution\n",
    "            x = torch.cat([x, feature], dim=1)  # concatenate skip features\n",
    "            x = block(x)  # pass through the block\n",
    "        return self.head(x)  # reduce to 1 channel\n",
    "\n",
    "\n",
    "def patch_accuracy_fn(y_hat, y):\n",
    "    # computes accuracy weighted by patches (metric used on Kaggle for evaluation)\n",
    "    h_patches = y.shape[-2] // PATCH_SIZE\n",
    "    w_patches = y.shape[-1] // PATCH_SIZE\n",
    "    patches_hat = y_hat.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
    "    patches = y.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
    "    return (patches == patches_hat).float().mean()\n",
    "\n",
    "def dice_similarity_fn(y_hat, y):\n",
    "    # computes dice similarity\n",
    "    overlap = (y_hat.round() == y.round()).float().sum()\n",
    "    return 2*overlap  / (2*overlap + y_hat.round().sum() + y.round().sum())\n",
    "\n",
    "def accuracy_fn(y_hat, y):\n",
    "    # computes classification accuracy\n",
    "    return (y_hat.round() == y.round()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb52d9-bdbd-4758-83ec-cc8ec767d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "chs=(3,64,128,256,512,1024)\n",
    "chs[::-1][:-1] # reverse it and remove first item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27325dfb-276c-4375-b626-691af2dfd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attn_unet import UTNet\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training using {device}\")\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset('training', device, use_patches=False, use_augmentation=True, use_inpainted_images=False, resize_to=(384, 384))\n",
    "val_dataset = ImageDataset('validation', device, use_patches=False, use_augmentation=False, resize_to=(384, 384))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "model = UTNet(chs=(3, 64, 128, 256), p_dropout=0.0).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn, 'dice': dice_similarity_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "n_epochs = 35\n",
    "train(train_dataloader, val_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e7e63-4456-422e-9db7-b728cda44abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_submission(labels, test_filenames, submission_filename):\n",
    "    test_path='test/images'\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for fn, patch_array in zip(sorted(test_filenames), test_pred):\n",
    "            img_number = int(re.search(r\"\\d+\", fn).group(0))\n",
    "            for i in range(patch_array.shape[0]):\n",
    "                for j in range(patch_array.shape[1]):\n",
    "                    f.write(\"{:03d}_{}_{},{}\\n\".format(img_number, j*PATCH_SIZE, i*PATCH_SIZE, int(patch_array[i, j])))\n",
    "\n",
    "test_path = 'test/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf56a54-0320-4416-9a3f-0a1e802db4c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(\"unet\"))\n",
    "\n",
    "# predict on test set\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0] # number of test images\n",
    "size = test_images.shape[1:3] #WH of images (currently its 400x400 -> reshape to 384x384 for model)\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in test_images], 0)\n",
    "test_images = test_images[:, :, :, :3] # only get first three channels\n",
    "test_images = np_to_tensor(np.moveaxis(test_images, -1, 1), device) # switch from HWC to CWH\n",
    "test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)] # use our model to predict segmentation mask\n",
    "test_pred = np.concatenate(test_pred, 0)\n",
    "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
    "\n",
    "# now test_pred has shape (144, 400, 400), for each image the pixel have values 0-1.\n",
    "\n",
    "# now compute labels\n",
    "test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE)) # split in patches\n",
    "test_pred = np.moveaxis(test_pred, 2, 3) # move dimension 16x16 together\n",
    "test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF) # mean of 16x16, classify 0 or 1\n",
    "create_submission(test_pred, test_filenames, submission_filename='unet_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e51a3-b47e-432a-aa20-f4a3f16f3047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
